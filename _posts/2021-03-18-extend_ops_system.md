---
layout: post
title: 如何设计一个可靠的、易扩展与维护的系统
image: /img/data_system.png
tag: [数据中台,数据系统]
---
Talk is not cheap. Talk can be powerful.    ——来自网络

## 可靠的系统

我们如何设计一个可靠的系统呢？这是所有应用开发人员都要考虑的问题。早些年的时候，新闻里都时不时的报道很多因为应用系统的不可靠导致公司、政府机构、甚至现如今的商家，当然还有一些个体户遭受损失。我并没有危言耸听，因为这样的错误，不可避免地，无时无刻地发生着，或许你觉得离你很远，但是在如今这个万物互联的社会没有一个人能逃脱。首先，声明一下。在这篇内容里，我不讨论因为hack导致的系统宕机以及服务不可用，这样一个属于系统安全范畴的题目，应该由系统安全人员来给我们讲述，如何和hacker斗智斗勇。

我们可以思考一下，什么才是系统的“可靠"？难道仅仅是因为系统能够顺利的运行，没有明显的BUG？或者稳定不宕机，能处理大量的QPS？这些似乎只是KPI要考虑的事情。不是一个正当的讨论议题。就像你不能把所有的学科讨论最终只是落实到如何找到一份好的工作。以前，我们总喜欢把拥有数据库、缓存、队列这样的功能分别归于不同的系统实现。我们习惯于在一个系统里只做一件事情。因为我们并不想花大量的时间来扩展其系统的其他能力。如果我们实现应用的场景只是停留在博客时代，那面对现如今我们这样的一个数据爆发的年代，过去的老旧系统就显得力不从心了。我很讨厌在一个系统里只做一件事情，这种既浪费资源也违背客观规律的做法，着实让我很难接受。在系统设计上除了着眼于核心功能之间的关联性和稳定性，甚至可以附加一些额外的功能操作也不为过，比如，可以使用Redis实现pub/sub的方式， Apache Kafka也借鉴了mysql binlog原理来持久化队列的消息。

近年来，随着数据成为企业开发的灵魂，可以毫不夸张地讲，现如今开发的系统都是“数据系统"。如今的企业不再是产生数据的机器，而是需要会采集、处理、分析数据。数据与数据之间的交换，处理、分析能力显得更加紧迫。此刻，单一组件的系统就只能被强迫集成较多有数据处理能力的功能，久而久之这单一的组件会变得越来越庞大以至于很难维护和扩展。当然，此时的数据系统会变得很不可靠，我们可能会因为一个小小BUG而浪费大量的资源来恢复，当然这里也包含了人力的资源。

那么，如果你问我什么是一个可靠的系统？我觉得有几句话总结就够了，可靠的系统应具备，可达到的预期的结果，不可预知操作的容错能力及关键数据的恢Ï复能力，防止未经授权的访问和滥用以及适应不同场景的系统负载能力。当然这些都会被看作是，即使是发生了错误，系统也照常可以运行。

```
```

![我们希望数据系统具有的能力](/img/data_system.png)

## 影响因素

### 硬件故障

由于云计算的迅速发展，很多企业都很乐意将服务部署在公有云上。而这些云计算提供商背后却是超大规模的机房和数据中心。这些机房和数据中心都是需要强有力的硬件支持。想象一下，如果因为硬件的错误，如硬盘的奔溃、内存故障、电网的停电，以及有人恶意拔网线，这种引起的宕机却是灾难性的，甚至是不可恢复的。前几年，某云平台因运营和硬盘故障导致其用户数据的丢失，也让其面临很昂贵的赔偿。

![阿里宕机](/img/ali_down.png)

有研究表明一个硬盘的平均无故障的时间约10年～50年。这就说明，你硬盘里那50G的内容也不得不面临被自我销毁的可能。有研究表明，在一个有10000个硬盘存储集群中，每天大概有一个磁盘发生故障。当然，我们同样会采用冗余组件来替换出错的硬件,好在如今的磁盘和内存制作成本的低廉，相比于云计算市场丰厚的利润，这样的损耗率也是忽略不计的。这样的方式可以让我们的服务器稳定的运行数年。但这些都没有办法从根本上解决问题。因为难以预知的硬件问题还是比较多。很多云技术提供商都会提供备用方案，比如两地三中心的机器架构，普遍地采用机房备用的电源、发电机。但即使在硬件上能做到差强人意，我们也很难避免在网络传输过程中，发生的一系列错误，而这些错误却需要软件层面来进行补足。

### 软件的BUG


### 不可抗力因素


## 易扩展

## 易维护

## 总结
